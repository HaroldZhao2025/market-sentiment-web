name: Build Data & Deploy Site

on:
  push:
    branches: ["main"]
  workflow_dispatch:
    inputs:
      mode:
        description: "Run mode: sample (AAPL) or full (S&P500)"
        required: true
        default: "sample"
        type: choice
        options: ["sample", "full"]
      yfinance_count:
        description: "Yahoo Finance items per ticker"
        required: false
        default: "240"
        type: choice
        options: ["120", "180", "200", "220", "240"]
      finnhub_rps:
        description: "Finnhub requests per second (≤30)"
        required: false
        default: "8"
        type: choice
        options: ["5", "8", "10", "15", "20", "30"]
      max_days_per_ticker:
        description: "Historical backfill days per ticker (Finnhub daily)"
        required: false
        default: "365"
        type: choice
        options: ["120", "180", "240", "300", "365"]
  schedule:
    - cron: "0 * * * *"   # refresh hourly
    - cron: "0 12 * * 1"  # plus Mondays 12:00 UTC

permissions:
  contents: write
  pages: write
  id-token: write

concurrency:
  group: pages
  cancel-in-progress: true

env:
  PYTHONPATH: src
  HF_HUB_DISABLE_TELEMETRY: "1"
  NEXT_PUBLIC_BASE_PATH: "/market-sentiment-web"

jobs:
  build:
    runs-on: ubuntu-latest
    timeout-minutes: 90

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          persist-credentials: true

      # ---------- Python & deps ----------
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-${{ runner.os }}-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            pip-${{ runner.os }}-

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip wheel setuptools
          pip install --no-cache-dir "torch==2.3.1" --index-url https://download.pytorch.org/whl/cpu
          pip install --no-cache-dir -r requirements.txt
          pip install --no-cache-dir finnhub-python yfinance transformers pandas requests

      - name: Configure HF cache
        run: echo "HF_HOME=$RUNNER_TEMP/hf-home" >> "$GITHUB_ENV"

      - name: Ensure base folders (avoid rsync/build errors)
        run: |
          mkdir -p data
          mkdir -p apps/web/public/data/ticker

      - name: Set date range (last 365 days)
        shell: python
        run: |
          import os
          from datetime import datetime, timedelta, timezone
          start=(datetime.now(timezone.utc)-timedelta(days=365)).strftime('%Y-%m-%d')
          end=datetime.now(timezone.utc).strftime('%Y-%m-%d')
          with open(os.environ['GITHUB_ENV'],'a') as f:
              f.write(f"START={start}\nEND={end}\n")
          print("START", start, "END", end)

      - name: Select mode and universe
        shell: bash
        run: |
          set -euo pipefail
          MODE="${{ github.event.inputs.mode }}"
          if [ -z "${MODE:-}" ]; then MODE="sample"; fi
          if [ "$MODE" = "full" ]; then
            TICKER_CSV="data/sp500.csv"
          else
            TICKER_CSV="data/sample_one.csv"
            printf "ticker\nAAPL\n" > "$TICKER_CSV"
          fi
          echo "MODE=$MODE" >> "$GITHUB_ENV"
          echo "TICKER_CSV=$TICKER_CSV" >> "$GITHUB_ENV"
          echo "Mode: $MODE | Universe: $TICKER_CSV"

      - name: Generate S&P500 CSV if needed (MODE=full; multi-source, 403-safe)
        if: env.MODE == 'full'
        shell: python
        run: |
          import os, sys, pandas as pd, requests
          from pathlib import Path

          outp = Path("data/sp500.csv")
          if outp.exists() and outp.stat().st_size > 0:
              print(f"{outp} already exists; skipping fetch.")
              sys.exit(0)

          UA = {"User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 Safari/537.36"}
          candidates = []

          try:
              print("Trying Wikipedia…")
              r = requests.get("https://en.wikipedia.org/wiki/List_of_S%26P_500_companies", headers=UA, timeout=30)
              r.raise_for_status()
              tables = pd.read_html(r.text)
              df = next((t for t in tables if "Symbol" in t.columns), None)
              if df is not None:
                  s = df["Symbol"].astype(str)
                  candidates.append(s)
                  print(f"Wikipedia ok: {len(s)} symbols")
          except Exception as e:
              print("Wikipedia failed:", e)

          try:
              print("Trying Slickcharts…")
              r = requests.get("https://www.slickcharts.com/sp500", headers=UA, timeout=30)
              r.raise_for_status()
              tables = pd.read_html(r.text)
              df = next((t for t in tables if "Symbol" in t.columns), tables[0] if tables else None)
              if df is not None:
                  col = "Symbol" if "Symbol" in df.columns else df.columns[0]
                  s = df[col].astype(str)
                  candidates.append(s)
                  print(f"Slickcharts ok: {len(s)} symbols")
          except Exception as e:
              print("Slickcharts failed:", e)

          for url in [
              "https://raw.githubusercontent.com/datasets/s-and-p-500-companies/main/data/constituents.csv",
              "https://raw.githubusercontent.com/datasets/s-and-p-500-companies/master/data/constituents.csv",
          ]:
              try:
                  print(f"Trying dataset: {url}")
                  df = pd.read_csv(url)
                  col = "Symbol" if "Symbol" in df.columns else ("symbol" if "symbol" in df.columns else None)
                  if col:
                      s = df[col].astype(str)
                      candidates.append(s)
                      print(f"Dataset ok: {len(s)} symbols from {url}")
                      break
              except Exception as e:
                  print(f"Dataset failed ({url}):", e)

          if not candidates:
              raise RuntimeError("All S&P500 sources failed (403/parse). Commit data/sp500.csv to repo to bypass network.")

          ser = pd.concat(candidates, ignore_index=True)
          ser = (ser.str.upper().str.replace(".", "-", regex=False).str.strip())
          uniq = sorted(set(x for x in ser if x))
          outp.parent.mkdir(parents=True, exist_ok=True)
          outp.write_text("ticker\n" + "\n".join(uniq) + "\n", encoding="utf-8")
          print(f"Wrote {len(uniq)} tickers to {outp}")

      # ---------- Historical Finnhub (token rotation + rate-limit) ----------
      - name: Backfill Finnhub per-day JSON (rotate tokens on 429, respect RPS)
        env:
          FINNHUB_TOKENS: ${{ secrets.FINNHUB_TOKENS }}
          FINNHUB_TOKEN:  ${{ secrets.FINNHUB_TOKEN }}
          FINNHUB_TOKEN_2: ${{ secrets.FINNHUB_TOKEN_2 }}
          FINNHUB_RPS: ${{ github.event.inputs.finnhub_rps }}
          MAX_DAYS_PER_TICKER: ${{ github.event.inputs.max_days_per_ticker }}
        continue-on-error: true
        shell: python
        run: |
          import os, time, json, pandas as pd
          from pathlib import Path
          try:
              import finnhub
          except Exception:
              finnhub = None

          START = os.environ["START"]; END = os.environ["END"]
          FINNHUB_RPS = max(1, min(30, int(os.environ.get("FINNHUB_RPS") or "8")))
          MAX_DAYS = int(os.environ.get("MAX_DAYS_PER_TICKER") or "365")
          TICKER_CSV = os.environ["TICKER_CSV"]

          def build_tokens() -> list[str]:
              raw = os.environ.get("FINNHUB_TOKENS", "").strip()
              toks = [x.strip() for x in raw.split(",") if x.strip()] if raw else []
              if not toks:
                  t1 = os.environ.get("FINNHUB_TOKEN", "").strip()
                  t2 = os.environ.get("FINNHUB_TOKEN_2", "").strip()
                  toks = [x for x in [t1, t2] if x]
              return toks

          TOKENS = build_tokens()
          if finnhub is None or not TOKENS:
              print("Finnhub missing or no tokens; skipping.")
              raise SystemExit(0)

          class TokenPool:
              def __init__(self, tokens):
                  self.tokens = tokens[:]
                  self.idx = 0
              def current(self):
                  if self.idx >= len(self.tokens): return None
                  return self.tokens[self.idx]
              def rotate(self):
                  self.idx += 1
                  return self.current()

          pool = TokenPool(TOKENS)
          client = finnhub.Client(api_key=pool.current())

          def read_universe(csv_path: str) -> list[str]:
              import pandas as pd
              df = pd.read_csv(csv_path)
              col = [c for c in df.columns if c.lower()=="ticker"]
              if not col: return []
              return [str(x).strip().upper().replace(".", "-") for x in df[col[0]].dropna().unique().tolist()]

          s_all = pd.Timestamp(START, tz="UTC").normalize()
          e_all = pd.Timestamp(END, tz="UTC").normalize()
          days = list(pd.date_range(s_all, e_all, freq="D", tz="UTC"))
          if MAX_DAYS and len(days) > MAX_DAYS:
              days = days[-MAX_DAYS:]

          min_gap = 1.0 / float(FINNHUB_RPS)
          last_call = 0.0

          def save_json(path: Path, obj):
              path.parent.mkdir(parents=True, exist_ok=True
              )
              with path.open("w", encoding="utf-8") as f:
                  json.dump(obj, f, ensure_ascii=False, indent=2)

          for t in read_universe(TICKER_CSV):
              base = Path(f"data/{t}/news/finnhub")
              base.mkdir(parents=True, exist_ok=True)
              logp = Path(f"data/{t}/logs"); logp.mkdir(parents=True, exist_ok=True)
              logf = (logp / "finnhub_fetch.log").open("a", encoding="utf-8")

              wrote = 0; skipped = 0
              for dts in days:
                  day = dts.date().isoformat()
                  out = base / f"{day}.json"
                  if out.exists() and out.stat().st_size > 0:
                      skipped += 1
                      continue

                  now = time.time()
                  wait = max(0.0, (last_call + min_gap) - now)
                  if wait > 0: time.sleep(wait)

                  tries, arr = 0, []
                  while tries < 5:
                      try:
                          arr = client.company_news(t, _from=day, to=day) or []
                          break
                      except Exception as e:
                          msg = str(e)
                          if ("429" in msg) or ("API limit reached" in msg) or ("Remaining Limit: 0" in msg):
                              logf.write(f"[FH] {t} {day} rate-limited: {msg}\n")
                              nxt = pool.rotate()
                              if nxt:
                                  client = finnhub.Client(api_key=nxt)
                                  time.sleep(1.0)
                                  tries += 1
                                  continue
                              else:
                                  backoff = min(16.0, 2.0 ** tries)
                                  time.sleep(backoff)
                                  tries += 1
                                  if tries >= 5:
                                      break
                          else:
                              logf.write(f"[FH] {t} {day} error: {msg}\n")
                              break

                  last_call = time.time()

                  cleaned = []
                  for it in arr:
                      try:
                          ts = pd.Timestamp.utcfromtimestamp(int(it.get("datetime"))).tz_localize("UTC").isoformat()
                      except Exception:
                          ts = None
                      cleaned.append({
                        "ts": ts,
                        "headline": (it.get("headline") or "").strip(),
                        "summary": (it.get("summary") or it.get("headline") or "").strip(),
                        "url": it.get("url") or "",
                        "source": it.get("source") or "",
                        "provider": "finnhub",
                        "raw": it
                      })
                  save_json(out, cleaned)
                  wrote += 1

              logf.write(f"[FH] {t}: wrote {wrote} files, skipped {skipped}\n")
              logf.close()
              print(f"[FH] {t}: wrote {wrote} files, skipped {skipped}")

      # ---------- Yahoo Finance (~200 recent) per-day JSON ----------
      - name: Save Yahoo Finance per-day JSON (~200)
        env:
          YF_COUNT: ${{ github.event.inputs.yfinance_count }}
        shell: python
        run: |
          import os, json, pandas as pd, yfinance as yf
          from pathlib import Path

          START=os.environ["START"]; END=os.environ["END"]
          TICKER_CSV=os.environ["TICKER_CSV"]
          YF_COUNT = int(os.environ.get("YF_COUNT") or "240")

          def read_universe(csv_path: str) -> list[str]:
              df = pd.read_csv(csv_path)
              col = [c for c in df.columns if c.lower()=="ticker"]
              if not col: return []
              return [str(x).strip().upper().replace(".", "-") for x in df[col[0]].dropna().unique().tolist()]

          def norm_ts(v):
              if v is None: return None
              try:
                  vi = int(v)
                  if vi > 10_000_000_000: vi = vi/1000.0
                  return pd.Timestamp.utcfromtimestamp(vi).tz_localize("UTC")
              except Exception:
                  try:
                      return pd.to_datetime(v, utc=True, errors="coerce")
                  except Exception:
                      return None

          s = pd.to_datetime(START, utc=True)
          e = pd.to_datetime(END, utc=True) + pd.Timedelta(days=1) - pd.Timedelta(seconds=1)

          for t in read_universe(TICKER_CSV):
              items=[]
              try:
                  items = yf.Ticker(t).get_news(count=YF_COUNT, tab="all") or []
              except Exception:
                  pass

              rows=[]
              for it in items:
                  content = it.get("content", {})
                  ts = (norm_ts(it.get("providerPublishTime"))
                        or norm_ts(content.get("pubDate"))
                        or norm_ts(content.get("displayTime"))
                        or norm_ts(content.get("published")))
                  if ts is None or pd.isna(ts) or ts < s or ts > e:
                      continue
                  title = (content.get("title") or it.get("title") or "").strip()
                  if not title:
                      continue
                  url = ((content.get("canonicalUrl") or {}).get("url")
                         or (content.get("clickThroughUrl") or {}).get("url")
                         or it.get("link") or it.get("url") or "")
                  summary = (content.get("summary") or content.get("description") or it.get("summary") or title).strip()
                  source = (content.get("provider") or {}).get("displayName") if isinstance(content.get("provider"), dict) else "Yahoo"
                  rows.append({
                      "ts": ts.isoformat(),
                      "headline": title,
                      "summary": summary,
                      "url": url,
                      "source": source,
                      "provider": "yfinance",
                      "raw": it
                  })

              if not rows:
                  print(f"[YF] {t}: 0 items in window")
                  continue

              df = pd.DataFrame(rows)   # <-- FIXED: use pd.DataFrame
              for day, grp in df.groupby(df["ts"].str.slice(0,10)):
                  out = Path(f"data/{t}/news/yfinance/{day}.json")
                  out.parent.mkdir(parents=True, exist_ok=True)
                  new = grp.to_dict(orient="records")
                  if out.exists() and out.stat().st_size>0:
                      try:
                          old = json.load(open(out, encoding="utf-8"))
                      except Exception:
                          old = []
                      seen = {(o.get("headline",""),o.get("url","")) for o in old}
                      for r in new:
                          key=(r.get("headline",""),r.get("url",""))
                          if key not in seen:
                              old.append(r); seen.add(key)
                      new = old
                  json.dump(new, open(out,"w",encoding="utf-8"), ensure_ascii=False, indent=2)
              print(f"[YF] {t}: saved per-day JSON under data/{t}/news/yfinance")

      # ---------- Daily sentiment (FinBERT, 4 decimals) ----------
      - name: Compute daily sentiment from both sources
        shell: python
        run: |
          import os, json, glob, pandas as pd
          from pathlib import Path
          from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline

          START=os.environ["START"]; END=os.environ["END"]; TICKER_CSV=os.environ["TICKER_CSV"]
          model_name = "ProsusAI/finbert"
          tok = AutoTokenizer.from_pretrained(model_name)
          mdl = AutoModelForSequenceClassification.from_pretrained(model_name)
          clf = pipeline("sentiment-analysis", model=mdl, tokenizer=tok, return_all_scores=True, truncation=True)

          def read_universe(csv_path: str) -> list[str]:
              df = pd.read_csv(csv_path)
              col = [c for c in df.columns if c.lower()=="ticker"]
              if not col: return []
              return [str(x).strip().upper().replace(".", "-") for x in df[col[0]].dropna().unique().tolist()]

          s = pd.to_datetime(START, utc=True).date()
          e = pd.to_datetime(END, utc=True).date()

          for t in read_universe(TICKER_CSV):
              base = Path(f"data/{t}")
              (base/"sentiment").mkdir(parents=True, exist_ok=True)

              day_texts = {}
              for prov in ("finnhub","yfinance"):
                  nd = base/"news"/prov
                  if not nd.exists():
                      continue
                  for p in sorted(glob.glob(str(nd/"*.json"))):
                      day = Path(p).stem
                      try:
                          ddate = pd.to_datetime(day, utc=True).date()
                      except Exception:
                          continue
                      if ddate < s or ddate > e:
                          continue
                      try:
                          arr = json.load(open(p, encoding="utf-8")) or []
                      except Exception:
                          arr = []
                      rec = day_texts.setdefault(day, {"texts": [], "counts": {"finnhub":0, "yfinance":0}})
                      for it in arr:
                          title = (it.get("headline") or "").strip()
                          if title:
                              rec["texts"].append(title)
                              rec["counts"][prov] += 1

              for day, obj in day_texts.items():
                  out = base/"sentiment"/f"{day}.json"
                  if out.exists() and out.stat().st_size>0:
                      continue
                  texts = obj["texts"]
                  if not texts:
                      json.dump({"date": day, "ticker": t, "n_total": 0, "n_finnhub": 0, "n_yfinance": 0, "score_mean": 0.0}, open(out,"w"), indent=2)
                      continue
                  preds = clf(texts, batch_size=16)
                  vals=[]
                  for per in preds:
                      d = {x["label"].lower(): float(x["score"]) for x in per}
                      s_val = d.get("positive",0.0) - d.get("negative",0.0)
                      vals.append(s_val)
                  mean = round(float(pd.Series(vals).mean()), 4)
                  json.dump({
                    "date": day,
                    "ticker": t,
                    "n_total": len(texts),
                    "n_finnhub": obj["counts"]["finnhub"],
                    "n_yfinance": obj["counts"]["yfinance"],
                    "score_mean": mean
                  }, open(out,"w"), indent=2)
              print(f"[SENT] {t}: sentiment JSON written")


      # ---------- Aggregate news counts from sentiment (writes data/{T}/news/counts.json) ----------
      - name: Aggregate news counts (sentiment → counts.json)
        shell: python
        env:
          PYTHONPATH: src
        run: |
          from market_sentiment.build_news_counts import main
          main()

      # ---------- NEW: Daily close prices (Yahoo) ----------
      - name: Fetch daily close prices (Yahoo)
        shell: python
        run: |
          import os, json, pandas as pd, yfinance as yf
          from pathlib import Path
          START=os.environ["START"]; END=os.environ["END"]; TICKER_CSV=os.environ["TICKER_CSV"]

          def read_universe(csv_path: str) -> list[str]:
              df = pd.read_csv(csv_path)
              col = [c for c in df.columns if c.lower()=="ticker"]
              if not col: return []
              return [str(x).strip().upper().replace(".", "-") for x in df[col[0]].dropna().unique().tolist()]

          for t in read_universe(TICKER_CSV):
              try:
                  df = yf.download(t, start=START, end=END, interval="1d", progress=False, auto_adjust=False)
              except Exception as e:
                  print(f"[PRICE] {t}: download failed:", e); df = None
              if df is None or df.empty:
                  print(f"[PRICE] {t}: empty")
                  continue
              df = df.reset_index()
              if "Date" in df.columns:
                  df["date"] = pd.to_datetime(df["Date"], utc=True).dt.date.astype(str)
              else:
                  df["date"] = pd.to_datetime(df.index, utc=True).date.astype(str)
              df["close"] = df["Close"].astype(float)
              out = Path(f"data/{t}/price/daily.json")
              out.parent.mkdir(parents=True, exist_ok=True)
              json.dump([{"date": d, "close": float(c)} for d, c in zip(df["date"], df["close"])], open(out,"w"), indent=2)
              print(f"[PRICE] {t}: {len(df)} rows -> {out}")

      # ---------- Build site JSON (apps/web/public/data) ----------
      - name: Assemble site JSON (apps/web/public/data)
        shell: python
        run: |
          import os, json, glob, pandas as pd
          from pathlib import Path

          START=os.environ["START"]; END=os.environ["END"]; TICKER_CSV=os.environ["TICKER_CSV"]
          pub = Path("apps/web/public/data"); (pub/"ticker").mkdir(parents=True, exist_ok=True)

          def read_universe(csv_path: str) -> list[str]:
              df = pd.read_csv(csv_path)
              col = [c for c in df.columns if c.lower()=="ticker"]
              if not col: return []
              return [str(x).strip().upper().replace(".", "-") for x in df[col[0]].dropna().unique().tolist()]

          tickers = read_universe(TICKER_CSV)
          (pub/"_tickers.json").write_text(json.dumps(tickers, indent=2), encoding="utf-8")

          # lazy FinBERT loader (cache at module level)
          _clf = None
          def get_clf():
              global _clf   # <-- FIX: use global (not nonlocal)
              if _clf is not None:
                  return _clf
              try:
                  from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
                  model_name = "ProsusAI/finbert"
                  tok = AutoTokenizer.from_pretrained(model_name)
                  mdl = AutoModelForSequenceClassification.from_pretrained(model_name)
                  _clf = pipeline("sentiment-analysis", model=mdl, tokenizer=tok, return_all_scores=True, truncation=True)
              except Exception as e:
                  print("[SITE] FinBERT load failed; headlines won't have per-item scores:", e)
                  _clf = None
              return _clf

          def score_headlines(items):
              """Attach per-headline probs + scalar s in-place. Safe no-op if model unavailable."""
              if not items:
                  return
              clf = get_clf()
              if clf is None:
                  return
              texts = [(it.get("title") or "").strip() for it in items]
              try:
                  preds = clf(texts, batch_size=16)
              except Exception as e:
                  print("[SITE] FinBERT inference failed; skipping per-item scores:", e)
                  return
              for it, per in zip(items, preds):
                  try:
                      d = {x.get("label","").lower(): float(x.get("score", 0.0)) for x in per}
                      pos = d.get("positive", 0.0)
                      neu = d.get("neutral", 0.0)
                      neg = d.get("negative", 0.0)
                      it["probs"] = {"pos": round(pos, 4), "neu": round(neu, 4), "neg": round(neg, 4)}
                      it["s"] = round(pos - neg, 4)
                  except Exception:
                      pass

          for t in tickers:
              base = Path(f"data/{t}")

              # --- sentiment rows + aggregate news counts from sentiment files
              sfiles = sorted(glob.glob(str(base/"sentiment/*.json")))
              rows=[]; nf_total=0; ny_total=0; nt_total=0
              for sf in sfiles:
                  try:
                      o = json.load(open(sf, encoding="utf-8")) or {}
                  except Exception:
                      o = {}
                  d = o.get("date")
                  if not d: continue
                  rows.append((d, float(o.get("score_mean", 0.0))))
                  nf_total += int(o.get("n_finnhub", 0))
                  ny_total += int(o.get("n_yfinance", 0))
                  nt_total += int(o.get("n_total", 0))
              rows = sorted(rows, key=lambda x:x[0])
              dates = [d for d,_ in rows]
              S = [round(float(v), 4) for _,v in rows]

              # --- prices aligned to sentiment dates (ffill + bfill)
              price_json = base/"price"/"daily.json"
              pmap={}
              if price_json.exists():
                  try:
                      arr = json.load(open(price_json, encoding="utf-8")) or []
                      pmap = {str(a.get("date")): float(a.get("close")) for a in arr if a.get("date") is not None and a.get("close") is not None}
                  except Exception:
                      pmap = {}
              if dates and pmap:
                  idx = pd.Index(sorted(set(list(pmap.keys()) + dates)))
                  s = pd.Series(pmap).reindex(idx).sort_index().ffill().bfill()
                  price = [float(s.get(d)) for d in dates]
              else:
                  price = []

              # --- recent headline items (window-filtered)
              news_items=[]
              for prov in ("finnhub","yfinance"):
                  for p in glob.glob(str(base/f"news/{prov}/*.json")):
                      try:
                          arr = json.load(open(p, encoding="utf-8")) or []
                          for it in arr:
                              news_items.append({
                                "ts": it.get("ts"),
                                "title": it.get("headline") or "",
                                "url": it.get("url") or "",
                                "source": it.get("source") or prov,
                                "provider": prov
                              })
                      except Exception:
                          pass

              s = pd.to_datetime(START, utc=True)
              e = pd.to_datetime(END, utc=True) + pd.Timedelta(days=1) - pd.Timedelta(seconds=1)
              def in_window(ts):
                  try:
                      tt = pd.to_datetime(ts, utc=True)
                      return tt >= s and tt <= e
                  except Exception:
                      return False
              news_items = [n for n in news_items if n.get("ts") and in_window(n.get("ts"))]
              news_items.sort(key=lambda n: n["ts"], reverse=True)
              news_trim = news_items[:20]

              # Attach per-headline scores (adds n['s'] and n['probs'])
              score_headlines(news_trim)

              obj = {
                "ticker": t,
                "dates": dates,
                "price": [round(float(x), 4) for x in price] if price else [],
                "S": S,
                "sentiment": S,
                "news_count": {"finnhub": int(nf_total), "yfinance": int(ny_total), "total": int(nt_total)},
                "news": news_trim
              }
              (pub/"ticker"/f"{t}.json").write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding="utf-8")
              print(f"[SITE] wrote {pub/'ticker'/f'{t}.json'} (days={len(rows)} news={len(news_trim)} price={len(obj['price'])})")

      # ---------- Commit & push raw data even if ignored ----------
      - name: Commit & push data directory
        if: always()
        shell: bash
        run: |
          set -euo pipefail
          git config --global user.name  "github-actions[bot]"
          git config --global user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git config --global --add safe.directory "$GITHUB_WORKSPACE"
          mkdir -p data || true
          git add -f data/** || true
          git add -f data/*  || true
          if git diff --cached --quiet; then
            echo "No changes to commit in data/."
          else
            git commit -m "chore(data): persist news, daily sentiment, and prices [skip ci]"
            git pull --rebase || true
            git push || true
          fi

      # ---------- Node & web build ----------
      - name: Setup Node.js (with npm cache)
        if: ${{ hashFiles('apps/web/package-lock.json') != '' }}
        uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: "npm"
          cache-dependency-path: apps/web/package-lock.json

      - name: Setup Node.js (no cache)
        if: ${{ hashFiles('apps/web/package-lock.json') == '' }}
        uses: actions/setup-node@v4
        with:
          node-version: "20"

      - name: Install web deps (ensure public/data exists)
        working-directory: apps/web
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p public/data public/data/ticker
          if [ -f package-lock.json ]; then
            npm ci --no-audit --no-fund
          else
            npm install --no-audit --no-fund || true
          fi
          npm i -D @types/react @types/node --no-audit --no-fund || true

      - name: Build static site (synthesize out/ if needed)
        working-directory: apps/web
        env:
          NEXT_TELEMETRY_DISABLED: "1"
          NEXT_PUBLIC_BASE_PATH: ${{ env.NEXT_PUBLIC_BASE_PATH }}
        shell: bash
        run: |
          set -euo pipefail
          npm run build || true

          rm -rf out
          mkdir -p out

          if [ -d ".next/static" ]; then
            mkdir -p out/_next
            rsync -a .next/static/ out/_next/static/
          fi

          if [ -d "public" ]; then
            rsync -a public/ out/
          else
            mkdir -p public
            rsync -a public/ out/
          fi

          if [ -d ".next/server/app" ]; then
            find .next/server/app -type f -name 'index.html' -print0 | while IFS= read -r -d '' f; do
              rel="${f#.next/server/app}"
              dir="${rel%/index.html}"
              mkdir -p "out/${dir}"
              cp "$f" "out/${dir}/index.html"
            done
            find .next/server/app -type f -name '*.html' ! -name 'index.html' -print0 | while IFS= read -r -d '' f; do
              rel="${f#.next/server/app/}"
              rel_noext="${rel%.html}"
              mkdir -p "out/${rel_noext}"
              cp "$f" "out/${rel_noext}/index.html"
            done
          fi

          touch out/.nojekyll
          find out -maxdepth 3 -type f | sort

      - name: Configure Pages
        uses: actions/configure-pages@v5

      - name: Upload artifact for Pages
        uses: actions/upload-pages-artifact@v3
        with:
          path: apps/web/out

  deploy:
    needs: build
    runs-on: ubuntu-latest
    environment:
      name: github-pages
    steps:
      - name: Deploy to GitHub Pages
        uses: actions/deploy-pages@v4
