name: Build Data & Deploy Site

on:
  push:
    branches: ["main"]
  workflow_dispatch: {}
  schedule:
    # Hourly refresh (YF news + prices)
    - cron: "0 * * * *"
    # Daily (approx 16:15 ET across DST/Standard)
    - cron: "15 20 * * *"
    - cron: "15 21 * * *"

permissions:
  contents: write
  pages: write
  id-token: write

concurrency:
  group: pages
  cancel-in-progress: true

env:
  PYTHONPATH: src
  HF_HUB_DISABLE_TELEMETRY: "1"
  NEXT_PUBLIC_BASE_PATH: "/market-sentiment-web"
  YF_COUNT: "240"
  FINNHUB_RPS: "8"
  MAX_DAYS_PER_TICKER: "365"

jobs:
  build:
    runs-on: ubuntu-latest
    timeout-minutes: 90

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          persist-credentials: true

      - name: Classify run kind (hourly/daily/manual)
        run: |
          RK=manual
          if [ "${{ github.event_name }}" = "schedule" ]; then
            if [ "${{ github.event.schedule }}" = "0 * * * *" ]; then
              RK=hourly
            elif [ "${{ github.event.schedule }}" = "15 20 * * *" ] || [ "${{ github.event.schedule }}" = "15 21 * * *" ]; then
              RK=daily
            fi
          fi
          echo "RUN_KIND=$RK" >> "$GITHUB_ENV"
          echo "Run kind: $RK"

      # ---------- Python & deps ----------
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-${{ runner.os }}-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            pip-${{ runner.os }}-

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip wheel setuptools
          pip install --no-cache-dir "torch==2.3.1" --index-url https://download.pytorch.org/whl/cpu
          pip install --no-cache-dir -r requirements.txt
          pip install --no-cache-dir finnhub-python yfinance transformers pandas requests

      - name: Configure HF cache
        run: echo "HF_HOME=$RUNNER_TEMP/hf-home" >> "$GITHUB_ENV"

      - name: Ensure base folders
        run: |
          mkdir -p data
          mkdir -p apps/web/public/data/ticker

      - name: Set date range (last 365 days)
        shell: python
        run: |
          import os
          from datetime import datetime, timedelta, timezone
          start=(datetime.now(timezone.utc)-timedelta(days=365)).strftime('%Y-%m-%d')
          end=datetime.now(timezone.utc).strftime('%Y-%m-%d')
          with open(os.environ['GITHUB_ENV'],'a') as f:
              f.write(f"START={start}\nEND={end}\n")
          print("START", start, "END", end)

      # Universe
      - name: Ensure S&P 500 CSV (multi-source)
        shell: python
        run: |
          import sys, pandas as pd, requests
          from pathlib import Path
          outp = Path("data/sp500.csv")
          if outp.exists() and outp.stat().st_size > 0:
              print(f"{outp} already exists; skipping fetch.")
              sys.exit(0)
          UA = {"User-Agent": "Mozilla/5.0"}
          candidates = []
          try:
              r = requests.get("https://en.wikipedia.org/wiki/List_of_S%26P_500_companies", headers=UA, timeout=30)
              r.raise_for_status()
              tables = pd.read_html(r.text)
              df = next((t for t in tables if "Symbol" in t.columns), None)
              if df is not None:
                  candidates.append(df["Symbol"].astype(str))
          except Exception as e:
              print("Wikipedia failed:", e)
          try:
              r = requests.get("https://www.slickcharts.com/sp500", headers=UA, timeout=30)
              r.raise_for_status()
              tables = pd.read_html(r.text)
              df = next((t for t in tables if "Symbol" in t.columns), tables[0] if tables else None)
              if df is not None:
                  col = "Symbol" if "Symbol" in df.columns else df.columns[0]
                  candidates.append(df[col].astype(str))
          except Exception as e:
              print("Slickcharts failed:", e)
          for url in [
              "https://raw.githubusercontent.com/datasets/s-and-p-500-companies/main/data/constituents.csv",
              "https://raw.githubusercontent.com/datasets/s-and-p-500-companies/master/data/constituents.csv",
          ]:
              try:
                  df = pd.read_csv(url)
                  col = "Symbol" if "Symbol" in df.columns else ("symbol" if "symbol" in df.columns else None)
                  if col:
                      candidates.append(df[col].astype(str))
                      break
              except Exception as e:
                  print("Dataset failed:", e)
          if not candidates:
              raise RuntimeError("All S&P500 sources failed.")
          ser = pd.concat(candidates, ignore_index=True)
          ser = ser.str.upper().str.replace(".", "-", regex=False).str.strip()
          uniq = sorted(set(x for x in ser if x))
          outp.parent.mkdir(parents=True, exist_ok=True)
          outp.write_text("ticker\n" + "\n".join(uniq) + "\n", encoding="utf-8")
          print(f"Wrote {len(uniq)} tickers to {outp}")

      - name: Set universe path
        run: echo "TICKER_CSV=data/sp500.csv" >> "$GITHUB_ENV"

      # ---------- Yahoo Finance per-day JSON ----------
      - name: Save Yahoo Finance per-day JSON
        shell: python
        env:
          YF_COUNT: ${{ env.YF_COUNT }}
          RUN_KIND: ${{ env.RUN_KIND }}
        run: |
          import os, json, pandas as pd, yfinance as yf
          from pathlib import Path
          START=os.environ["START"]; END=os.environ["END"]; TICKER_CSV=os.environ["TICKER_CSV"]
          YF_COUNT=int(os.environ.get("YF_COUNT") or "240")
          RUN_KIND=os.environ.get("RUN_KIND","manual")
          def read_universe(p):
              df=pd.read_csv(p); col=[c for c in df.columns if c.lower()=="ticker"]; 
              return [str(x).strip().upper().replace(".","-") for x in df[col[0]].dropna().unique().tolist()] if col else []
          def norm_ts(v):
              if v is None: return None
              try:
                  vi=int(v); vi=vi/1000.0 if vi>10_000_000_000 else vi
                  return pd.Timestamp.utcfromtimestamp(vi).tz_localize("UTC")
              except: 
                  try: return pd.to_datetime(v, utc=True, errors="coerce")
                  except: return None
          e=pd.to_datetime(END, utc=True)+pd.Timedelta(days=1)-pd.Timedelta(seconds=1)
          s=pd.to_datetime(END, utc=True)-pd.Timedelta(days=3) if RUN_KIND=="hourly" else pd.to_datetime(START, utc=True)
          for t in read_universe(TICKER_CSV):
              items=[]
              try: items=yf.Ticker(t).get_news(count=YF_COUNT, tab="all") or []
              except: pass
              rows=[]
              for it in items:
                  if not isinstance(it, dict): continue
                  content = it.get("content") or {}
                  if not isinstance(content, dict): content = {}
                  ts=(norm_ts(it.get("providerPublishTime")) or norm_ts(content.get("pubDate")) or norm_ts(content.get("displayTime")) or norm_ts(content.get("published")))
                  if ts is None or pd.isna(ts) or ts<s or ts>e: continue
                  title=(content.get("title") or it.get("title") or "").strip()
                  if not title: continue
                  url=((content.get("canonicalUrl") or {}).get("url") or (content.get("clickThroughUrl") or {}).get("url") or it.get("link") or it.get("url") or "")
                  prov=content.get("provider")
                  source= (prov.get("displayName") or prov.get("name")) if isinstance(prov, dict) else (prov if isinstance(prov,str) else "Yahoo")
                  rows.append({"ts":ts.isoformat(),"headline":title,"summary":(content.get("summary") or content.get("description") or it.get("summary") or title).strip(),"url":url,"source":source,"provider":"yfinance","raw":it})
              if not rows:
                  print(f"[YF] {t}: 0 items in window"); continue
              df=pd.DataFrame(rows)
              for day, grp in df.groupby(df["ts"].str.slice(0,10)):
                  out=Path(f"data/{t}/news/yfinance/{day}.json")
                  out.parent.mkdir(parents=True, exist_ok=True)
                  new=grp.to_dict(orient="records")
                  if out.exists() and out.stat().st_size>0:
                      try: old=json.load(open(out, encoding="utf-8"))
                      except Exception: old=[]
                      seen={(o.get("headline",""),o.get("url","")) for o in old}
                      for r in new:
                          key=(r.get("headline",""),r.get("url",""))
                          if key not in seen: old.append(r); seen.add(key)
                      new=old
                  json.dump(new, open(out,"w",encoding="utf-8"), ensure_ascii=False, indent=2)
              print(f"[YF] {t}: saved per-day JSON")

      # ---------- Finnhub (daily/manual only) ----------
      - name: Backfill Finnhub per-day JSON
        if: ${{ env.RUN_KIND != 'hourly' }}
        env:
          FINNHUB_TOKENS: ${{ secrets.FINNHUB_TOKENS }}
          FINNHUB_TOKEN:  ${{ secrets.FINNHUB_TOKEN }}
          FINNHUB_TOKEN_2: ${{ secrets.FINNHUB_TOKEN_2 }}
          FINNHUB_RPS: ${{ env.FINNHUB_RPS }}
          MAX_DAYS_PER_TICKER: ${{ env.MAX_DAYS_PER_TICKER }}
        continue-on-error: true
        shell: python
        run: |
          import os, time, json, pandas as pd
          from pathlib import Path
          try:
              import finnhub
          except Exception:
              finnhub = None
          START=os.environ["START"]; END=os.environ["END"]; TICKER_CSV=os.environ["TICKER_CSV"]
          RPS=max(1,min(30,int(os.environ.get("FINNHUB_RPS") or "8")))
          MAX_DAYS=int(os.environ.get("MAX_DAYS_PER_TICKER") or "365")
          def read_universe(p):
              df=pd.read_csv(p); col=[c for c in df.columns if c.lower()=="ticker"]; 
              return [str(x).strip().upper().replace(".","-") for x in df[col[0]].dropna().unique().tolist()] if col else []
          def build_tokens():
              raw=(os.environ.get("FINNHUB_TOKENS","") or "").strip()
              toks=[x.strip() for x in raw.split(",") if x.strip()] if raw else []
              if not toks:
                  for k in ("FINNHUB_TOKEN","FINNHUB_TOKEN_2"):
                      v=(os.environ.get(k,"") or "").strip()
                      if v: toks.append(v)
              return toks
          TOKENS=build_tokens()
          if finnhub is None or not TOKENS:
              print("Finnhub missing or no tokens; skipping."); raise SystemExit(0)
          class Pool:
              def __init__(self,t): self.t=t; self.i=0
              def cur(self): return None if self.i>=len(self.t) else self.t[self.i]
              def rot(self): self.i+=1; return self.cur()
          pool=Pool(TOKENS); client=finnhub.Client(api_key=pool.cur())
          s_all=pd.Timestamp(START, tz="UTC").normalize()
          e_all=pd.Timestamp(END, tz="UTC").normalize()
          days=list(pd.date_range(s_all,e_all,freq="D",tz="UTC"))
          if MAX_DAYS and len(days)>MAX_DAYS: days=days[-MAX_DAYS:]
          gap=1.0/float(RPS); last=0.0
          for t in read_universe(TICKER_CSV):
              base=Path(f"data/{t}/news/finnhub"); base.mkdir(parents=True, exist_ok=True)
              logp=Path(f"data/{t}/logs"); logp.mkdir(parents=True, exist_ok=True)
              logf=(logp/"finnhub_fetch.log").open("a",encoding="utf-8")
              wrote=0; skipped=0
              for dts in days:
                  day=dts.date().isoformat(); out=base/f"{day}.json"
                  if out.exists() and out.stat().st_size>0: skipped+=1; continue
                  now=time.time(); wait=max(0.0,(last+gap)-now)
                  if wait>0: time.sleep(wait)
                  tries=0; arr=[]
                  while tries<5:
                      try:
                          arr=client.company_news(t,_from=day,to=day) or []; break
                      except Exception as e:
                          msg=str(e)
                          if "429" in msg or "API limit" in msg or "Remaining Limit: 0" in msg:
                              logf.write(f"[FH] {t} {day} rate-limited: {msg}\n")
                              nxt=pool.rot()
                              if nxt: client=finnhub.Client(api_key=nxt); time.sleep(1.0); tries+=1; continue
                              else:
                                  time.sleep(min(16.0,2.0**tries)); tries+=1
                                  if tries>=5: break
                          else:
                              logf.write(f"[FH] {t} {day} error: {msg}\n"); break
                  last=time.time()
                  cleaned=[]
                  for it in arr:
                      try: ts=pd.Timestamp.utcfromtimestamp(int(it.get("datetime"))).tz_localize("UTC").isoformat()
                      except Exception: ts=None
                      cleaned.append({"ts":ts,"headline":(it.get("headline") or "").strip(),"summary":(it.get("summary") or it.get("headline") or "").strip(),"url":it.get("url") or "","source":it.get("source") or "","provider":"finnhub","raw":it})
                  json.dump(cleaned, open(out,"w",encoding="utf-8"), ensure_ascii=False, indent=2)
                  wrote+=1
              logf.write(f"[FH] {t}: wrote {wrote} files, skipped {skipped}\n"); logf.close()
              print(f"[FH] {t}: wrote {wrote} files, skipped {skipped}")

      # ---------- Sentiment (daily/manual only) ----------
      - name: Compute daily sentiment from both sources
        if: ${{ env.RUN_KIND != 'hourly' }}
        shell: python
        run: |
          import os, json, glob, pandas as pd
          from pathlib import Path
          from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
          START=os.environ["START"]; END=os.environ["END"]; TICKER_CSV=os.environ["TICKER_CSV"]
          tok=AutoTokenizer.from_pretrained("ProsusAI/finbert")
          mdl=AutoModelForSequenceClassification.from_pretrained("ProsusAI/finbert")
          clf=pipeline("sentiment-analysis", model=mdl, tokenizer=tok, return_all_scores=True, truncation=True)
          def read_universe(p):
              df=pd.read_csv(p); col=[c for c in df.columns if c.lower()=="ticker"]; 
              return [str(x).strip().upper().replace(".","-") for x in df[col[0]].dropna().unique().tolist()] if col else []
          s=pd.to_datetime(START, utc=True).date(); e=pd.to_datetime(END, utc=True).date()
          for t in read_universe(TICKER_CSV):
              base=Path(f"data/{t}"); (base/"sentiment").mkdir(parents=True, exist_ok=True)
              day_texts={}
              for prov in ("finnhub","yfinance"):
                  nd=base/"news"/prov
                  if not nd.exists(): continue
                  for p in sorted(glob.glob(str(nd/"*.json"))):
                      day=Path(p).stem
                      try: ddate=pd.to_datetime(day, utc=True).date()
                      except Exception: continue
                      if ddate<s or ddate>e: continue
                      try: arr=json.load(open(p,encoding="utf-8")) or []
                      except Exception: arr=[]
                      rec=day_texts.setdefault(day, {"texts": [], "counts": {"finnhub":0, "yfinance":0}})
                      for it in arr:
                          title=(it.get("headline") or "").strip()
                          if title: rec["texts"].append(title); rec["counts"][prov]+=1
              for day, obj in day_texts.items():
                  out=base/"sentiment"/f"{day}.json"
                  if out.exists() and out.stat().st_size>0: continue
                  texts=obj["texts"]
                  if not texts:
                      json.dump({"date": day, "ticker": t, "n_total": 0, "n_finnhub": 0, "n_yfinance": 0, "score_mean": 0.0}, open(out,"w"), indent=2); continue
                  preds=clf(texts, batch_size=16)
                  vals=[]
                  for per in preds:
                      d={x["label"].lower(): float(x["score"]) for x in per}
                      vals.append(d.get("positive",0.0)-d.get("negative",0.0))
                  mean=round(float(pd.Series(vals).mean()), 4)
                  json.dump({"date": day,"ticker": t,"n_total": len(texts),"n_finnhub": obj["counts"]["finnhub"],"n_yfinance": obj["counts"]["yfinance"],"score_mean": mean}, open(out,"w"), indent=2)
              print(f"[SENT] {t}: sentiment JSON written")

      - name: Aggregate news counts
        shell: bash
        env:
          START: ${{ env.START }}
          END: ${{ env.END }}
          TICKER_CSV: ${{ env.TICKER_CSV }}
        run: |
          set -euo pipefail
          python -m market_sentiment.build_news_counts

      # ---------- Prices (merge, no overwrite) ----------
      - name: Fetch daily close prices (Yahoo)
        shell: python
        env:
          RUN_KIND: ${{ env.RUN_KIND }}
        run: |
          import os, json, pandas as pd, yfinance as yf
          from pathlib import Path
          START=os.environ["START"]; END=os.environ["END"]; TICKER_CSV=os.environ["TICKER_CSV"]
          RUN_KIND=os.environ.get("RUN_KIND","manual")
          def read_universe(p):
              df=pd.read_csv(p); col=[c for c in df.columns if c.lower()=="ticker"]; 
              return [str(x).strip().upper().replace(".","-") for x in df[col[0]].dropna().unique().tolist()] if col else []
          def all_news_days(t):
              days=[]
              for prov in ("finnhub","yfinance"):
                  nd=Path(f"data/{t}/news/{prov}")
                  if not nd.exists(): continue
                  for p in nd.glob("*.json"):
                      day=p.stem
                      try: pd.to_datetime(day, utc=True); days.append(day)
                      except Exception: pass
              return sorted(set(days))
          for t in read_universe(TICKER_CSV):
              path=Path(f"data/{t}/price/daily.json"); path.parent.mkdir(parents=True, exist_ok=True)
              exist=[]; 
              if path.exists() and path.stat().st_size>0:
                  try: exist=json.load(open(path,encoding="utf-8")) or []
                  except Exception: exist=[]
              pmap={str(r.get("date")): float(r.get("close")) for r in exist if r.get("date") and r.get("close") is not None}
              days=all_news_days(t)
              earliest_news = (pd.to_datetime(days[0]).date().isoformat() if days else START)
              if RUN_KIND=="hourly":
                  if pmap:
                      have_earliest=min(pd.to_datetime(d).date() for d in pmap)
                      en=pd.to_datetime(earliest_news).date()
                      fetch_start = (pd.to_datetime(END).date()-pd.Timedelta(days=14)).isoformat() if have_earliest<=en else earliest_news
                  else:
                      fetch_start=earliest_news
              else:
                  fetch_start=min(pd.to_datetime(START).date(), pd.to_datetime(earliest_news).date()).isoformat()
              try:
                  df=yf.download(t, start=fetch_start, end=END, interval="1d", progress=False, auto_adjust=False)
              except Exception as e:
                  print(f"[PRICE] {t}: download failed: {e}"); df=None
              if df is None or df.empty:
                  print(f"[PRICE] {t}: empty")
                  if pmap and not path.exists():
                      json.dump([{"date": d, "close": float(c)} for d,c in sorted(pmap.items())], open(path,"w"), indent=2)
                  continue
              df=df.reset_index()
              if "Date" in df.columns: df["date"]=pd.to_datetime(df["Date"], utc=True).dt.date.astype(str)
              else: df["date"]=pd.to_datetime(df.index, utc=True).date.astype(str)
              df["close"]=df["Close"].astype(float)
              for d,c in zip(df["date"], df["close"]): pmap[d]=float(c)
              out=[{"date": d, "close": float(c)} for d,c in sorted(pmap.items())]
              json.dump(out, open(path,"w"), indent=2)
              print(f"[PRICE] {t}: coverage {out[0]['date']} â†’ {out[-1]['date']} ({len(out)} rows) -> {path}")

      # ---------- WEB BUILD ----------
      # Ensure Next static export config exists (safe writer)
      - name: Ensure Next static export config
        working-directory: apps/web
        shell: bash
        run: |
          set -euo pipefail
          if [ ! -f "next.config.cjs" ]; then
            cat > next.config.cjs <<'EOF'
// Auto-generated by CI if missing
const base = process.env.NEXT_PUBLIC_BASE_PATH || "";
/** @type {import('next').NextConfig} */
module.exports = {
  output: "export",
  basePath: base,
  assetPrefix: base ? `${base}/` : "",
  trailingSlash: true,
  images: { unoptimized: true },
  reactStrictMode: true,
};
EOF
          fi
          node -e "require('fs').access('next.config.cjs', fs.constants.R_OK, (e)=>{ if(e){process.exit(1)} })"

      - name: Setup Node.js (with npm cache)
        if: ${{ hashFiles('apps/web/package-lock.json') != '' }}
        uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: "npm"
          cache-dependency-path: apps/web/package-lock.json

      - name: Setup Node.js (no cache)
        if: ${{ hashFiles('apps/web/package-lock.json') == '' }}
        uses: actions/setup-node@v4
        with:
          node-version: "20"

      - name: Install web deps (ensure public/data exists)
        working-directory: apps/web
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p public/data public/data/ticker
          if [ -f package-lock.json ]; then
            npm ci --no-audit --no-fund
          else
            npm install --no-audit --no-fund
          fi
          npm i -D @types/react @types/node --no-audit --no-fund

      - name: Build Next app
        working-directory: apps/web
        env:
          NEXT_TELEMETRY_DISABLED: "1"
          NEXT_PUBLIC_BASE_PATH: ${{ env.NEXT_PUBLIC_BASE_PATH }}
        shell: bash
        run: |
          set -euo pipefail
          npm run build

      - name: Synthesize static out/ from Next build
        working-directory: apps/web
        shell: bash
        run: |
          set -euo pipefail
          rm -rf out
          mkdir -p out
          if [ -d ".next/static" ]; then
            mkdir -p out/_next
            rsync -a .next/static/ out/_next/static/
          fi
          if [ -d "public" ]; then
            rsync -a public/ out/
          fi
          if [ -d ".next/server/app" ]; then
            find .next/server/app -type f -name 'index.html' -print0 | while IFS= read -r -d '' f; do
              rel="${f#.next/server/app}"
              dir="${rel%/index.html}"
              mkdir -p "out/${dir}"
              cp "$f" "out/${dir}/index.html"
            done
            find .next/server/app -type f -name '*.html' ! -name 'index.html' -print0 | while IFS= read -r -d '' f; do
              rel="${f#.next/server/app/}"
              rel_noext="${rel%.html}"
              mkdir -p "out/${rel_noext}"
              cp "$f" "out/${rel_noext}/index.html"
            done
          fi
          touch out/.nojekyll
          echo "Listing first few files under out/:"
          find out -maxdepth 3 -type f | sort | head -n 50

      # ---------- Persist raw data ----------
      - name: Commit & push data directory
        if: always()
        shell: bash
        run: |
          set -euo pipefail
          git config --global user.name  "github-actions[bot]"
          git config --global user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git config --global --add safe.directory "$GITHUB_WORKSPACE"
          mkdir -p data || true
          git add -f data/** || true
          git add -f data/*  || true
          if git diff --cached --quiet; then
            echo "No changes to commit in data/."
          else
            git commit -m "chore(data): refresh news/sentiment/prices (merge) and assemble site JSON [skip ci]"
            git pull --rebase || true
            git push || true
          fi

      - name: Configure Pages
        uses: actions/configure-pages@v5

      - name: Upload artifact for Pages
        uses: actions/upload-pages-artifact@v3
        with:
          path: apps/web/out

  deploy:
    needs: build
    runs-on: ubuntu-latest
    environment:
      name: github-pages
    steps:
      - name: Deploy to GitHub Pages
        uses: actions/deploy-pages@v4
