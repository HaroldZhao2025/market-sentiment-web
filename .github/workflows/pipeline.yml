name: Build Data & Deploy Site

on:
  push:
    branches: ["main"]
  workflow_dispatch:
    inputs:
      mode:
        description: "Run mode: sample (AAPL) or full (S&P500)"
        required: true
        default: "sample"
        type: choice
        options: ["sample", "full"]
      yfinance_count:
        description: "Yahoo Finance items per ticker"
        required: false
        default: "240"
        type: choice
        options: ["120", "180", "200", "220", "240"]
      finnhub_rps:
        description: "Finnhub requests per second (≤30)"
        required: false
        default: "10"
        type: choice
        options: ["5", "8", "10", "15", "20", "30"]
  schedule:
    - cron: "0 12 * * 1" # Mondays 12:00 UTC

permissions:
  contents: write
  pages: write
  id-token: write

concurrency:
  group: pages
  cancel-in-progress: true

env:
  PYTHONPATH: src
  HF_HUB_DISABLE_TELEMETRY: "1"
  NEXT_PUBLIC_BASE_PATH: "/market-sentiment-web"
  FINNHUB_TOKEN: ${{ secrets.FINNHUB_TOKEN }}

jobs:
  build:
    runs-on: ubuntu-latest
    timeout-minutes: 75

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          persist-credentials: true
          fetch-depth: 0

      # ---------- Python & deps ----------
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-${{ runner.os }}-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            pip-${{ runner.os }}-

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip wheel setuptools
          pip install --no-cache-dir "torch==2.3.1" --index-url https://download.pytorch.org/whl/cpu
          pip install --no-cache-dir -r requirements.txt
          pip install --no-cache-dir finnhub-python yfinance

      - name: Configure HF cache
        run: echo "HF_HOME=$RUNNER_TEMP/hf-home" >> "$GITHUB_ENV"

      - name: Set date range (last 365 days)
        shell: python
        run: |
          import os
          from datetime import datetime, timedelta, timezone
          start=(datetime.now(timezone.utc)-timedelta(days=365)).strftime('%Y-%m-%d')
          end=datetime.now(timezone.utc).strftime('%Y-%m-%d')
          with open(os.environ['GITHUB_ENV'],'a') as f:
              f.write(f"START={start}\nEND={end}\n")
          print("START", start, "END", end)

      - name: Propagate inputs to env
        shell: bash
        run: |
          echo "YF_COUNT=${{ github.event.inputs.yfinance_count || '240' }}" >> "$GITHUB_ENV"
          echo "FINNHUB_RPS=${{ github.event.inputs.finnhub_rps || '10' }}" >> "$GITHUB_ENV"

      - name: Ensure repo data directories (persisted history)
        run: |
          mkdir -p data
          mkdir -p data/history/news
          mkdir -p data/history/sentiment
          mkdir -p data/history/prices
          ls -la data || true

      - name: Select mode and universe
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p data
          MODE="${{ github.event.inputs.mode }}"
          if [ -z "${MODE:-}" ]; then MODE="sample"; fi
          if [ "$MODE" = "full" ]; then
            TICKER_CSV="data/sp500.csv"
          else
            TICKER_CSV="data/sample_one.csv"
            printf "ticker\nAAPL\n" > "$TICKER_CSV"
          fi
          echo "MODE=$MODE" >> "$GITHUB_ENV"
          echo "TICKER_CSV=$TICKER_CSV" >> "$GITHUB_ENV"
          echo "Mode: $MODE | Universe: $TICKER_CSV"

      - name: Generate S&P500 CSV if needed (MODE=full; multi-source, 403-safe)
        if: env.MODE == 'full'
        shell: python
        run: |
          import os, sys, pandas as pd, requests
          from pathlib import Path

          outp = Path("data/sp500.csv")
          if outp.exists() and outp.stat().st_size > 0:
              print(f"{outp} already exists; skipping fetch.")
              sys.exit(0)

          UA = {"User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 Safari/537.36"}
          candidates = []

          try:
              print("Trying Wikipedia…")
              r = requests.get("https://en.wikipedia.org/wiki/List_of_S%26P_500_companies", headers=UA, timeout=30)
              r.raise_for_status()
              tables = pd.read_html(r.text)
              df = next((t for t in tables if "Symbol" in t.columns), None)
              if df is not None:
                  s = df["Symbol"].astype(str)
                  candidates.append(s)
                  print(f"Wikipedia ok: {len(s)} symbols")
          except Exception as e:
              print("Wikipedia failed:", e)

          try:
              print("Trying Slickcharts…")
              r = requests.get("https://www.slickcharts.com/sp500", headers=UA, timeout=30)
              r.raise_for_status()
              tables = pd.read_html(r.text)
              df = next((t for t in tables if "Symbol" in t.columns), None) or (tables[0] if tables else None)
              if df is not None:
                  col = "Symbol" if "Symbol" in df.columns else df.columns[0]
                  s = df[col].astype(str)
                  candidates.append(s)
                  print(f"Slickcharts ok: {len(s)} symbols")
          except Exception as e:
              print("Slickcharts failed:", e)

          for url in [
              "https://raw.githubusercontent.com/datasets/s-and-p-500-companies/main/data/constituents.csv",
              "https://raw.githubusercontent.com/datasets/s-and-p-500-companies/master/data/constituents.csv",
          ]:
              try:
                  print(f"Trying dataset: {url}")
                  df = pd.read_csv(url)
                  col = "Symbol" if "Symbol" in df.columns else ("symbol" if "symbol" in df.columns else None)
                  if col:
                      s = df[col].astype(str)
                      candidates.append(s)
                      print(f"Dataset ok: {len(s)} symbols from {url}")
                      break
              except Exception as e:
                  print(f"Dataset failed ({url}):", e)

          if not candidates:
              raise RuntimeError("All S&P500 sources failed. Commit data/sp500.csv to repo to bypass network.")

          ser = pd.concat(candidates, ignore_index=True)
          ser = ser.str.upper().str.replace(".", "-", regex=False).str.strip()
          uniq = sorted(set(x for x in ser if x))
          outp.parent.mkdir(parents=True, exist_ok=True)
          outp.write_text("ticker\n" + "\n".join(uniq) + "\n", encoding="utf-8")
          print(f"Wrote {len(uniq)} tickers to {outp}")

      # ---------- Historical backfill: Finnhub (daily) + yfinance (latest ~200) ----------
      - name: Historical backfill to data/history (rate-limited, idempotent)
        env:
          FINNHUB_TOKEN: ${{ secrets.FINNHUB_TOKEN }}
        shell: python
        run: |
          import os, sys, time, json, pathlib, pandas as pd, datetime as dt
          import yfinance as yf
          try:
            import finnhub
          except Exception:
            finnhub = None

          START = os.environ["START"]
          END   = os.environ["END"]
          TICKER_CSV = os.environ["TICKER_CSV"]
          YF_COUNT = int(os.environ.get("YF_COUNT","240"))
          RPS = max(1, min(30, int(os.environ.get("FINNHUB_RPS","10"))))
          MIN_GAP = 1.0 / RPS
          MAX_TICKERS = int(os.environ.get("MAX_TICKERS","500"))

          def ensure_dir(p): pathlib.Path(p).mkdir(parents=True, exist_ok=True)

          tickers = pd.read_csv(TICKER_CSV)["ticker"].astype(str).str.upper().tolist()[:MAX_TICKERS]
          print("Backfill tickers:", len(tickers))

          token = os.environ.get("FINNHUB_TOKEN")
          client = finnhub.Client(api_key=token) if (token and finnhub is not None) else None
          if client is None:
            print("WARN: Finnhub token missing or finnhub not installed; skipping finnhub backfill.", file=sys.stderr)

          days = list(pd.date_range(pd.Timestamp(START, tz="UTC").normalize(),
                                    pd.Timestamp(END, tz="UTC").normalize(),
                                    freq="D"))

          last_call = 0.0
          def call_finnhub(tkr, dstr):
              nonlocal last_call
              now = time.time()
              wait = max(0.0, (last_call + MIN_GAP) - now)
              if wait > 0: time.sleep(wait)
              last_call = time.time()
              return client.company_news(tkr, _from=dstr, to=dstr) or []

          for tkr in tickers:
              tdir = f"data/history/news/{tkr}"
              ensure_dir(tdir)

              # yfinance latest
              try:
                  t = yf.Ticker(tkr)
                  try:
                      items = t.get_news(count=YF_COUNT, tab="all")
                  except TypeError:
                      items = t.news or []
                  with open(f"{tdir}/yfinance.latest.json", "w", encoding="utf-8") as f:
                      json.dump(items or [], f, ensure_ascii=False)
                  print(f"[yfinance] {tkr}: {len(items or [])} items saved -> {tdir}/yfinance.latest.json")
              except Exception as e:
                  print(f"[yfinance] {tkr}: ERROR {e}", file=sys.stderr)

              # finnhub per-day files
              if client is None:
                  continue
              wrote = 0
              for d in days:
                  dstr = d.date().isoformat()
                  outp = f"{tdir}/{dstr}.finnhub.json"
                  if os.path.exists(outp) and os.path.getsize(outp) > 2:
                      continue
                  try:
                      arr = call_finnhub(tkr, dstr)
                      if not isinstance(arr, list):
                          arr = []
                      with open(outp, "w", encoding="utf-8") as f:
                          json.dump(arr, f, ensure_ascii=False)
                      wrote += 1
                  except Exception as e:
                      msg = str(e)
                      if "429" in msg:
                          time.sleep(2.0)
                          try:
                              arr = call_finnhub(tkr, dstr)
                              with open(outp, "w", encoding="utf-8") as f:
                                  json.dump(arr or [], f, ensure_ascii=False)
                              wrote += 1
                          except Exception as e2:
                              print(f"[finnhub] {tkr} {dstr}: second attempt failed {e2}", file=sys.stderr)
                      else:
                          print(f"[finnhub] {tkr} {dstr}: ERROR {e}", file=sys.stderr)
              print(f"[finnhub] {tkr}: wrote {wrote} day-files into {tdir}")

      # ---------- Smoke tests ----------
      - name: Smoke test — yfinance (count from input)
        shell: python
        run: |
          import os, yfinance as yf
          c = int(os.environ.get("YF_COUNT","240"))
          t = yf.Ticker("AAPL")
          try:
            items = t.get_news(count=c, tab="all")
          except TypeError:
            items = t.news or []
          print(f"yfinance.get_news(count={c}, tab='all') -> {len(items or [])} items")

      - name: Smoke test — Finnhub single-day (reads history if present)
        shell: python
        run: |
          import os, json, datetime as dt
          day = (dt.datetime.utcnow()-dt.timedelta(days=1)).date().isoformat()
          path = f"data/history/news/AAPL/{day}.finnhub.json"
          if os.path.exists(path) and os.path.getsize(path) > 2:
              print(f"Found historical Finnhub file: {path}")
              with open(path, encoding="utf-8") as f:
                  arr = json.load(f)
              print(f"Items: {len(arr)}")
              for it in (arr[:3] if isinstance(arr, list) else []):
                  print(it.get("headline") or it.get("title") or "")
          else:
              print(f"No historical Finnhub file for {day} (this is OK if first run).")

      - name: Smoke test — AAPL prices
        shell: python
        run: |
          from market_sentiment.prices import fetch_prices_yf
          p = fetch_prices_yf("AAPL", "${{ env.START }}", "${{ env.END }}")
          print("AAPL prices rows:", len(p))
          if not p.empty:
            print(p.head(2).to_string(index=False))
            print(p.tail(2).to_string(index=False))

      # ---------- Build JSON data ----------
      - name: Build JSON data (prices + news)
        env:
          FINNHUB_TOKEN: ${{ secrets.FINNHUB_TOKEN }}
        shell: bash
        run: |
          set -euo pipefail
          echo "Build JSON for $(wc -l < "$TICKER_CSV" | tr -d ' ') tickers | yfinance_count=${YF_COUNT:-240} finnhub_rps=${FINNHUB_RPS:-10}"
          python -m market_sentiment.cli.build_json \
            --universe "$TICKER_CSV" \
            --start "$START" --end "$END" \
            --out apps/web/public/data \
            --batch 16 \
            --cutoff-minutes 5 \
            --max-workers 3 \
            --cache-dir data/history \
            --yfinance-count "${YF_COUNT:-240}" \
            --finnhub-rps "${FINNHUB_RPS:-10}"

      - name: Persist historical sentiment (CSV) into data/history/sentiment
        shell: python
        run: |
          import os, json, pandas as pd, pathlib
          base="apps/web/public/data/ticker"
          outdir="data/history/sentiment"
          pathlib.Path(outdir).mkdir(parents=True, exist_ok=True)
          if not os.path.isdir(base):
              raise SystemExit("No ticker data folder found.")
          for fn in sorted(os.listdir(base)):
              if not fn.endswith(".json"): 
                  continue
              tkr = fn[:-5]
              j = json.load(open(os.path.join(base, fn)))
              S = j.get("S") or j.get("sentiment") or []
              days = j.get("days") or j.get("dates") or []
              if not S or not days: 
                  continue
              df = pd.DataFrame({"date": days, "S": S})
              df["MA7"] = pd.Series(df["S"]).rolling(7, min_periods=1).mean().round(4)
              df.to_csv(os.path.join(outdir, f"{tkr}.csv"), index=False)
              print(f"Wrote {tkr} -> {outdir}/{tkr}.csv")

      - name: Commit & push data/history changes
        if: always()
        run: |
          set -e
          git config user.name  "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add -A data/history || true
          if git diff --cached --quiet; then
            echo "No changes to commit in data/history."
          else:
            git commit -m "chore(ci): backfill news + persist historical sentiment"
            git push
          fi

      # ---------- Node & web build ----------
      - name: Setup Node.js (with npm cache)
        if: ${{ hashFiles('apps/web/package-lock.json') != '' }}
        uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: "npm"
          cache-dependency-path: apps/web/package-lock.json

      - name: Setup Node.js (no cache)
        if: ${{ hashFiles('apps/web/package-lock.json') == '' }}
        uses: actions/setup-node@v4
        with:
          node-version: "20"

      - name: Install web deps (ci if lockfile, else install)
        working-directory: apps/web
        shell: bash
        run: |
          set -euo pipefail
          if [ -f package-lock.json ]; then
            npm ci --no-audit --no-fund
          else
            npm install --no-audit --no-fund
          fi
          npm i -D @types/react @types/node --no-audit --no-fund || true

      - name: Build static site (create out/ from .next)
        working-directory: apps/web
        env:
          NEXT_TELEMETRY_DISABLED: "1"
          NEXT_PUBLIC_BASE_PATH: ${{ env.NEXT_PUBLIC_BASE_PATH }}
        shell: bash
        run: |
          set -euo pipefail
          echo "==== next.config.cjs ===="
          cat next.config.cjs || true
          echo "==== Node/NPM ===="
          node -v
          npm -v

          npm run build

          echo "==== Prepare out/ ===="
          if [ -d "out" ]; then
            echo "Next produced out/ already. Using it."
          else
            echo "No out/ from Next; synthesizing from .next artifacts..."
            rm -rf out
            mkdir -p out

            if [ -d ".next/static" ]; then
              mkdir -p out/_next
              rsync -a .next/static out/_next/static
            fi
            rsync -a public/ out/
            if [ -d ".next/server/app" ]; then
              find .next/server/app -type f -name 'index.html' -print0 | while IFS= read -r -d '' f; do
                rel="${f#.next/server/app}"
                dir="${rel%/index.html}"
                mkdir -p "out/${dir}"
                cp "$f" "out/${dir}/index.html"
              done
              find .next/server/app -type f -name '*.html' ! -name 'index.html' -print0 | while IFS= read -r -d '' f; do
                rel="${f#.next/server/app/}"
                rel_noext="${rel%.html}"
                mkdir -p "out/${rel_noext}"
                cp "$f" "out/${rel_noext}/index.html"
              done
            fi
            if [ -f ".next/server/app/_not-found.html" ]; then
              cp ".next/server/app/_not-found.html" "out/404.html" || true
            elif [ ! -f "out/404.html" ] && [ -f "out/index.html" ]; then
              cp "out/index.html" "out/404.html" || true
            fi
          fi

          touch out/.nojekyll
          echo "==== Final out/ listing ===="
          find out -maxdepth 3 -type f | sort

      - name: Configure Pages
        uses: actions/configure-pages@v5

      - name: Upload artifact for Pages
        uses: actions/upload-pages-artifact@v3
        with:
          path: apps/web/out

  deploy:
    needs: build
    runs-on: ubuntu-latest
    environment:
      name: github-pages
    steps:
      - name: Deploy to GitHub Pages
        uses: actions/deploy-pages@v4
