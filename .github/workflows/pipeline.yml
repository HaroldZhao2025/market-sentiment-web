name: Build Data & Deploy Site (Queued, 1 ticker/run, skip A & AAPL)

on:
  push:
    branches: ["main"]
  workflow_dispatch: {}
  repository_dispatch:
    types: [ticker-queue-next]

permissions:
  contents: write
  pages: write
  id-token: write
  actions: write

concurrency:
  group: pages
  cancel-in-progress: true

env:
  PYTHONPATH: src
  HF_HUB_DISABLE_TELEMETRY: "1"
  NEXT_PUBLIC_BASE_PATH: "/market-sentiment-web"
  # Full-run knobs
  YF_COUNT: "240"
  FINNHUB_RPS: "8"
  MAX_DAYS_PER_TICKER: "365"

jobs:
  build:
    runs-on: ubuntu-latest
    timeout-minutes: 90

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          persist-credentials: true

      # ---------- Python & deps ----------
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-${{ runner.os }}-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            pip-${{ runner.os }}-

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip wheel setuptools
          pip install --no-cache-dir "torch==2.3.1" --index-url https://download.pytorch.org/whl/cpu
          pip install --no-cache-dir -r requirements.txt
          pip install --no-cache-dir finnhub-python yfinance transformers pandas requests

      - name: Configure HF cache
        run: echo "HF_HOME=$RUNNER_TEMP/hf-home" >> "$GITHUB_ENV"

      - name: Ensure base folders
        run: |
          mkdir -p data
          mkdir -p apps/web/public/data/ticker

      - name: Set date range (last 365 days)
        shell: python
        run: |
          import os
          from datetime import datetime, timedelta, timezone
          start=(datetime.now(timezone.utc)-timedelta(days=365)).strftime('%Y-%m-%d')
          end=datetime.now(timezone.utc).strftime('%Y-%m-%d')
          with open(os.environ['GITHUB_ENV'],'a') as f:
              f.write(f"START={start}\nEND={end}\n")
          print("START", start, "END", end)

      # ---- Universe file (create if missing) ----
      - name: Generate S&P500 CSV if needed
        shell: python
        run: |
          import os, sys, pandas as pd, requests
          from pathlib import Path

          outp = Path("data/sp500.csv")
          if outp.exists() and outp.stat().st_size > 0:
              print(f"{outp} exists; skip.")
              sys.exit(0)

          UA={"User-Agent":"Mozilla/5.0"}
          cands=[]

          try:
              r=requests.get("https://en.wikipedia.org/wiki/List_of_S%26P_500_companies",headers=UA,timeout=30); r.raise_for_status()
              tables=pd.read_html(r.text); df=next((t for t in tables if "Symbol" in t.columns), None)
              if df is not None: cands.append(df["Symbol"].astype(str))
          except Exception as e:
              print("Wikipedia failed:", e)

          try:
              r=requests.get("https://www.slickcharts.com/sp500",headers=UA,timeout=30); r.raise_for_status()
              tables=pd.read_html(r.text); df=next((t for t in tables if "Symbol" in t.columns), tables[0] if tables else None)
              if df is not None:
                  col="Symbol" if "Symbol" in df.columns else df.columns[0]
                  cands.append(df[col].astype(str))
          except Exception as e:
              print("Slickcharts failed:", e)

          if not cands:
              outp.write_text("ticker\nAAPL\n", encoding="utf-8")
              sys.exit(0)

          ser=pd.concat(cands, ignore_index=True).str.upper().str.replace(".","-",regex=False).str.strip()
          uniq=sorted(set(x for x in ser if x))
          outp.parent.mkdir(parents=True, exist_ok=True)
          outp.write_text("ticker\n"+"\n".join(uniq)+"\n", encoding="utf-8")
          print(f"Wrote {len(uniq)} tickers -> {outp}")

      # ---- Pick ONE ticker sequentially (skip A & AAPL), always overwrite ----
      - name: Pick next ticker (sequential; skip A & AAPL)
        id: pick
        shell: python
        run: |
          from pathlib import Path
          SKIP = {"A", "AAPL"}  # skip set

          sp = Path("data/sp500.csv")
          if not sp.exists():
              Path("data/_one.csv").write_text("ticker\nAAPL\n", encoding="utf-8")
              print("Universe missing; default to AAPL (will be skipped later if needed).")

          tickers = []
          if sp.exists():
              for line in sp.read_text(encoding="utf-8").splitlines()[1:]:
                  t=line.strip().upper().replace(".","-")
                  if t and t not in SKIP:
                      tickers.append(t)

          # progress pointer
          ptr = Path("data/_last_ticker.txt")
          last = ptr.read_text(encoding="utf-8").strip().upper() if ptr.exists() else ""

          next_t = ""
          if not tickers:
              next_t = ""
          else:
              if last and last in tickers:
                  idx = tickers.index(last) + 1
              else:
                  idx = 0
              next_t = tickers[idx] if idx < len(tickers) else ""

          with open("/home/runner/work/_temp/next_env", "w") as f:
              f.write(f"NEXT_TICKER={next_t}\n")
              if next_t:
                  f.write("TICKER_CSV=data/_one.csv\n")
          print("NEXT_TICKER:", next_t or "(none)")

          if next_t:
              Path("data/_one.csv").write_text("ticker\n"+next_t+"\n", encoding="utf-8")

          # Export env
          from shutil import move
          move("/home/runner/work/_temp/next_env", "/home/runner/work/_temp/_github_env")
          print("::add-mask::"+(next_t or ""))
        shell: bash

      - name: Load NEXT_TICKER env
        run: |
          if [ -f /home/runner/work/_temp/_github_env ]; then
            cat /home/runner/work/_temp/_github_env >> "$GITHUB_ENV"
          fi

      - name: Stop early if nothing to build
        if: env.NEXT_TICKER == ''
        run: echo "All sequential tickers completed (skipping A & AAPL). Nothing to do."

      # ---------- Finnhub (daily) ----------
      - name: Backfill Finnhub per-day JSON (one ticker)
        if: env.NEXT_TICKER != ''
        env:
          FINNHUB_TOKENS: ${{ secrets.FINNHUB_TOKENS }}
          FINNHUB_TOKEN:  ${{ secrets.FINNHUB_TOKEN }}
          FINNHUB_TOKEN_2: ${{ secrets.FINNHUB_TOKEN_2 }}
          FINNHUB_RPS: ${{ env.FINNHUB_RPS }}
          MAX_DAYS_PER_TICKER: ${{ env.MAX_DAYS_PER_TICKER }}
          START: ${{ env.START }}
          END: ${{ env.END }}
          TICKER_CSV: ${{ env.TICKER_CSV }}
        shell: python
        run: |
          import os, time, json, pandas as pd
          from pathlib import Path
          try:
              import finnhub
          except Exception:
              finnhub = None

          START=os.environ["START"]; END=os.environ["END"]
          FINNHUB_RPS=max(1, min(30, int(os.environ.get("FINNHUB_RPS") or "8")))
          MAX_DAYS=int(os.environ.get("MAX_DAYS_PER_TICKER") or "365")
          TICKER_CSV=os.environ["TICKER_CSV"]

          def read_universe(csv_path: str) -> list[str]:
              df=pd.read_csv(csv_path)
              col=[c for c in df.columns if c.lower()=="ticker"]
              if not col: return []
              return [str(x).strip().upper().replace(".","-") for x in df[col[0]].dropna().unique().tolist()]

          tokens=[]
          raw=os.environ.get("FINNHUB_TOKENS","").strip()
          if raw: tokens=[x.strip() for x in raw.split(",") if x.strip()]
          if not tokens:
              for k in ["FINNHUB_TOKEN","FINNHUB_TOKEN_2"]:
                  v=os.environ.get(k,"").strip()
                  if v: tokens.append(v)

          if finnhub is None or not tokens:
              print("Finnhub missing or no tokens; skipping."); raise SystemExit(0)

          class Pool:
              def __init__(self, toks): self.toks=toks; self.i=0
              def cur(self): return self.toks[self.i] if self.i < len(self.toks) else None
              def rot(self): self.i+=1; return self.cur()

          pool=Pool(tokens)
          client=finnhub.Client(api_key=pool.cur())

          s_all=pd.Timestamp(START, tz="UTC").normalize()
          e_all=pd.Timestamp(END, tz="UTC").normalize()
          days=list(pd.date_range(s_all, e_all, freq="D", tz="UTC"))
          if MAX_DAYS and len(days)>MAX_DAYS: days=days[-MAX_DAYS:]

          min_gap=1.0/float(FINNHUB_RPS); last=0.0
          def save_json(pth, obj):
              pth.parent.mkdir(parents=True, exist_ok=True)
              with pth.open("w", encoding="utf-8") as f: json.dump(obj, f, ensure_ascii=False, indent=2)

          for t in read_universe(TICKER_CSV):
              base=Path(f"data/{t}/news/finnhub"); base.mkdir(parents=True, exist_ok=True)
              wrote=0; skipped=0
              for d in days:
                  day=d.date().isoformat(); out=base/f"{day}.json"
                  now=time.time(); wait=max(0.0, (last+min_gap)-now); 
                  if wait>0: time.sleep(wait)
                  tries=0; arr=[]
                  while tries<5:
                      try:
                          arr=client.company_news(t, _from=day, to=day) or []; break
                      except Exception as e:
                          m=str(e)
                          if "429" in m or "Remaining Limit: 0" in m or "API limit" in m:
                              if not pool.rot(): time.sleep(min(16.0,2.0**tries))
                              else: client=finnhub.Client(api_key=pool.cur()); time.sleep(1.0)
                              tries+=1; continue
                          else:
                              print("Finnhub error:", m); break
                  last=time.time()
                  cleaned=[]
                  for it in arr:
                      try:
                          ts=pd.Timestamp.utcfromtimestamp(int(it.get("datetime"))).tz_localize("UTC").isoformat()
                      except Exception:
                          ts=None
                      cleaned.append({
                        "ts": ts,
                        "headline": (it.get("headline") or "").strip(),
                        "summary": (it.get("summary") or it.get("headline") or "").strip(),
                        "url": it.get("url") or "",
                        "source": it.get("source") or "",
                        "provider": "finnhub",
                        "raw": it
                      })
                  save_json(out, cleaned); wrote+=1
              print(f"[FH] {t}: wrote {wrote}, skipped {skipped}")

      # ---------- Yahoo Finance (robust) ----------
      - name: Save Yahoo Finance per-day JSON (one ticker)
        if: env.NEXT_TICKER != ''
        shell: python
        env:
          START: ${{ env.START }}
          END: ${{ env.END }}
          TICKER_CSV: ${{ env.TICKER_CSV }}
          YF_COUNT: ${{ env.YF_COUNT }}
        run: |
          import os, json, pandas as pd, yfinance as yf
          from pathlib import Path

          START=os.environ["START"]; END=os.environ["END"]
          TICKER_CSV=os.environ["TICKER_CSV"]
          YF_COUNT=int(os.environ.get("YF_COUNT") or "240")

          def read_universe(csv_path: str) -> list[str]:
              df=pd.read_csv(csv_path)
              col=[c for c in df.columns if c.lower()=="ticker"]
              if not col: return []
              return [str(x).strip().upper().replace(".","-") for x in df[col[0]].dropna().unique().tolist()]

          def norm_ts(v):
              if v is None: return None
              try:
                  vi=int(v)
                  if vi>10_000_000_000: vi=vi/1000.0
                  return pd.Timestamp.utcfromtimestamp(vi).tz_localize("UTC")
              except Exception:
                  try: return pd.to_datetime(v, utc=True, errors="coerce")
                  except Exception: return None

          s=pd.to_datetime(START, utc=True)
          e=pd.to_datetime(END, utc=True)+pd.Timedelta(days=1)-pd.Timedelta(seconds=1)

          for t in read_universe(TICKER_CSV):
              items=[]
              try: items=yf.Ticker(t).get_news(count=YF_COUNT, tab="all") or []
              except Exception: pass

              rows=[]
              for it in items:
                  if not isinstance(it, dict): continue
                  content = it.get("content") or {}
                  if not isinstance(content, dict): content = {}

                  ts = (norm_ts(it.get("providerPublishTime"))
                        or norm_ts(content.get("pubDate"))
                        or norm_ts(content.get("displayTime"))
                        or norm_ts(content.get("published")))
                  if ts is None or pd.isna(ts) or ts < s or ts > e: continue

                  title=(content.get("title") or it.get("title") or "").strip()
                  if not title: continue

                  url=((content.get("canonicalUrl") or {}).get("url")
                       or (content.get("clickThroughUrl") or {}).get("url")
                       or it.get("link") or it.get("url") or "")

                  prov=content.get("provider")
                  if isinstance(prov, dict):
                      source=prov.get("displayName") or prov.get("name") or "Yahoo"
                  elif isinstance(prov, str):
                      source=prov
                  else:
                      source="Yahoo"

                  rows.append({
                      "ts": ts.isoformat(),
                      "headline": title,
                      "summary": (content.get("summary") or content.get("description") or it.get("summary") or title).strip(),
                      "url": url,
                      "source": source,
                      "provider": "yfinance",
                      "raw": it
                  })

              if not rows:
                  print(f"[YF] {t}: 0 items in window"); continue

              df=pd.DataFrame(rows)
              for day, grp in df.groupby(df["ts"].str.slice(0,10)):
                  out=Path(f"data/{t}/news/yfinance/{day}.json")
                  out.parent.mkdir(parents=True, exist_ok=True)
                  new=grp.to_dict(orient="records")
                  if out.exists() and out.stat().st_size>0:
                      try: old=json.load(open(out, encoding="utf-8"))
                      except Exception: old=[]
                      seen={(o.get("headline",""),o.get("url","")) for o in old}
                      for r in new:
                          key=(r.get("headline",""),r.get("url",""))
                          if key not in seen:
                              old.append(r); seen.add(key)
                      new=old
                  json.dump(new, open(out,"w",encoding="utf-8"), ensure_ascii=False, indent=2)
              print(f"[YF] {t}: saved per-day JSON under data/{t}/news/yfinance")

      # ---------- Daily sentiment (FinBERT) ----------
      - name: Compute daily sentiment (one ticker)
        if: env.NEXT_TICKER != ''
        shell: python
        env:
          START: ${{ env.START }}
          END: ${{ env.END }}
          TICKER_CSV: ${{ env.TICKER_CSV }}
        run: |
          import os, json, glob, pandas as pd
          from pathlib import Path
          from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline

          START=os.environ["START"]; END=os.environ["END"]; TICKER_CSV=os.environ["TICKER_CSV"]
          tok=AutoTokenizer.from_pretrained("ProsusAI/finbert")
          mdl=AutoModelForSequenceClassification.from_pretrained("ProsusAI/finbert")
          clf=pipeline("sentiment-analysis", model=mdl, tokenizer=tok, return_all_scores=True, truncation=True)

          def read_universe(csv_path: str) -> list[str]:
              df=pd.read_csv(csv_path)
              col=[c for c in df.columns if c.lower()=="ticker"]
              if not col: return []
              return [str(x).strip().upper().replace(".","-") for x in df[col[0]].dropna().unique().tolist()]

          s=pd.to_datetime(START, utc=True).date()
          e=pd.to_datetime(END, utc=True).date()

          for t in read_universe(TICKER_CSV):
              base=Path(f"data/{t}"); (base/"sentiment").mkdir(parents=True, exist_ok=True)
              day_texts={}
              for prov in ("finnhub","yfinance"):
                  nd=base/"news"/prov
                  if not nd.exists(): continue
                  for p in sorted(glob.glob(str(nd/"*.json"))):
                      day=Path(p).stem
                      try: ddate=pd.to_datetime(day, utc=True).date()
                      except Exception: continue
                      if ddate<s or ddate>e: continue
                      try: arr=json.load(open(p, encoding="utf-8")) or []
                      except Exception: arr=[]
                      rec=day_texts.setdefault(day, {"texts": [], "counts": {"finnhub":0,"yfinance":0}})
                      for it in arr:
                          title=(it.get("headline") or "").strip()
                          if title:
                              rec["texts"].append(title)
                              rec["counts"][prov]+=1

              for day, obj in day_texts.items():
                  out=base/"sentiment"/f"{day}.json"
                  texts=obj["texts"]
                  if not texts:
                      json.dump({"date":day,"ticker":t,"n_total":0,"n_finnhub":0,"n_yfinance":0,"score_mean":0.0}, open(out,"w"), indent=2)
                      continue
                  preds=clf(texts, batch_size=16)
                  vals=[]
                  for per in preds:
                      d={x["label"].lower(): float(x["score"]) for x in per}
                      vals.append(d.get("positive",0.0)-d.get("negative",0.0))
                  mean=round(float(pd.Series(vals).mean()),4)
                  json.dump({
                    "date": day, "ticker": t,
                    "n_total": len(texts),
                    "n_finnhub": obj["counts"]["finnhub"],
                    "n_yfinance": obj["counts"]["yfinance"],
                    "score_mean": mean
                  }, open(out,"w"), indent=2)
              print(f"[SENT] {t}: daily sentiment written")

      # ---------- Aggregate news counts ----------
      - name: Aggregate news counts
        if: env.NEXT_TICKER != ''
        shell: bash
        env:
          START: ${{ env.START }}
          END: ${{ env.END }}
          TICKER_CSV: ${{ env.TICKER_CSV }}
        run: |
          set -euo pipefail
          python -m market_sentiment.build_news_counts

      # ---------- Prices ----------
      - name: Fetch daily close prices (one ticker)
        if: env.NEXT_TICKER != ''
        shell: python
        env:
          START: ${{ env.START }}
          END: ${{ env.END }}
          TICKER_CSV: ${{ env.TICKER_CSV }}
        run: |
          import os, json, pandas as pd, yfinance as yf
          from pathlib import Path
          START=os.environ["START"]; END=os.environ["END"]; TICKER_CSV=os.environ["TICKER_CSV"]

          def read_universe(csv_path: str) -> list[str]:
              df=pd.read_csv(csv_path)
              col=[c for c in df.columns if c.lower()=="ticker"]
              if not col: return []
              return [str(x).strip().upper().replace(".","-") for x in df[col[0]].dropna().unique().tolist()]

          for t in read_universe(TICKER_CSV):
              try:
                  df=yf.download(t, start=START, end=END, interval="1d", progress=False, auto_adjust=False)
              except Exception as e:
                  print(f"[PRICE] {t}: download failed: {e}"); df=None
              if df is None or df.empty:
                  print(f"[PRICE] {t}: empty"); continue
              df=df.reset_index()
              if "Date" in df.columns:
                  df["date"]=pd.to_datetime(df["Date"], utc=True).dt.date.astype(str)
              else:
                  df["date"]=pd.to_datetime(df.index, utc=True).date.astype(str)
              df["close"]=df["Close"].astype(float)
              out=Path(f"data/{t}/price/daily.json")
              out.parent.mkdir(parents=True, exist_ok=True)
              json.dump([{"date":d,"close":float(c)} for d,c in zip(df["date"], df["close"])], open(out,"w"), indent=2)
              print(f"[PRICE] {t}: {len(df)} rows -> {out}")

      # ---------- Assemble site JSON ----------
      - name: Assemble site JSON (apps/web/public/data)
        if: env.NEXT_TICKER != ''
        shell: python
        env:
          START: ${{ env.START }}
          END: ${{ env.END }}
          TICKER_CSV: ${{ env.TICKER_CSV }}
        run: |
          import os, json, glob
          from pathlib import Path
          import pandas as pd
          from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline

          START=os.environ["START"]; END=os.environ["END"]; TICKER_CSV=os.environ["TICKER_CSV"]
          pub=Path("apps/web/public/data"); (pub/"ticker").mkdir(parents=True, exist_ok=True)

          _tok=AutoTokenizer.from_pretrained("ProsusAI/finbert")
          _mdl=AutoModelForSequenceClassification.from_pretrained("ProsusAI/finbert")
          _clf=pipeline("sentiment-analysis", model=_mdl, tokenizer=_tok, top_k=None, truncation=True)

          def score_one(title:str):
              try:
                  raw=_clf(title)[0]
                  m={d["label"].lower(): float(d["score"]) for d in raw}
                  pos,neu,neg=m.get("positive",0.0), m.get("neutral",0.0), m.get("negative",0.0)
                  s=round(pos-neg,4)
                  if s>=0.4: lab="Strong Positive"
                  elif s>=0.1: lab="Positive"
                  elif s<=-0.4: lab="Strong Negative"
                  elif s<=-0.1: lab="Negative"
                  else: lab="Neutral"
                  return s, lab
              except Exception:
                  return None, None

          s_win=pd.to_datetime(START, utc=True)
          e_win=pd.to_datetime(END, utc=True)+pd.Timedelta(days=1)-pd.Timedelta(seconds=1)

          # Universe here is just the one ticker in _one.csv
          import pandas as pd
          df=pd.read_csv(TICKER_CSV)
          tickers=[str(x).strip().upper().replace(".","-") for x in df[df.columns[0]].tolist()[1:]]

          for t in tickers:
              base=Path(f"data/{t}")

              # sentiment timeline
              sfiles=sorted(glob.glob(str(base/"sentiment/*.json")))
              rows=[]; nf_total=0; ny_total=0; nt_total=0
              for sf in sfiles:
                  try: o=json.load(open(sf, encoding="utf-8")) or {}
                  except Exception: o={}
                  day=o.get("date"); 
                  if not day: continue
                  rows.append((day, float(o.get("score_mean", 0.0))))
                  nf_total+=int(o.get("n_finnhub",0) or 0)
                  ny_total+=int(o.get("n_yfinance",0) or 0)
                  nt_total+=int(o.get("n_total",0) or 0)

              rows=sorted(rows, key=lambda x:x[0])
              dates=[d for d,_ in rows]
              S=[round(float(v),4) for _,v in rows]

              # price aligned
              price_json=base/"price"/"daily.json"; price=[]
              if price_json.exists():
                  try:
                      arr=json.load(open(price_json, encoding="utf-8")) or []
                      pmap={str(a.get("date")): float(a.get("close")) for a in arr if a.get("date") and a.get("close") is not None}
                  except Exception:
                      pmap={}
                  if dates and pmap:
                      idx=pd.Index(sorted(set(list(pmap.keys())+dates)))
                      sser=pd.Series(pmap).reindex(idx).sort_index().ffill().bfill()
                      price=[float(sser.get(d)) for d in dates]

              # 10 recent headlines (window)
              news_items=[]
              for prov in ("finnhub","yfinance"):
                  for p in glob.glob(str(base/f"news/{prov}/*.json")):
                      try:
                          arr=json.load(open(p, encoding="utf-8")) or []
                          for it in arr:
                              ts=it.get("ts")
                              if not ts: continue
                              try:
                                  tt=pd.to_datetime(ts, utc=True)
                                  if not (s_win<=tt<=e_win): continue
                              except Exception:
                                  continue
                              news_items.append({
                                "ts": ts,
                                "title": it.get("headline") or it.get("title") or "",
                                "url": it.get("url") or "",
                                "source": it.get("source") or prov,
                                "provider": prov
                              })
                      except Exception: pass
              news_items.sort(key=lambda n: n["ts"], reverse=True)
              news_trim=news_items[:10]

              # per-headline score
              scored=[]
              for item in news_trim:
                  if not item.get("title"): scored.append(item); continue
                  s,lab=score_one(item["title"])
                  if s is not None: item["s"]=s
                  if lab is not None: item["sentiment_label"]=lab
                  scored.append(item)
              if scored:
                  print(f"[SITE] {t}: scored {len(scored)}/{len(news_trim)} headlines")

              obj={
                "ticker": t,
                "dates": dates,
                "price": [round(float(x),4) for x in price] if price else [],
                "S": S,
                "sentiment": S,
                "news_count": {"finnhub": int(nf_total), "yfinance": int(ny_total), "total": int(nt_total)},
                "news_total": int(nt_total),
                "news": scored
              }
              (pub/"ticker"/f"{t}.json").write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding="utf-8")
              print(f"[SITE] wrote {pub/'ticker'/f'{t}.json'} (days={len(rows)} news={len(scored)} price={len(obj['price'])})")

      # ---------- Update pointer to LAST processed ticker ----------
      - name: Update last ticker pointer
        if: env.NEXT_TICKER != ''
        run: |
          echo "${NEXT_TICKER}" > data/_last_ticker.txt
          cat data/_last_ticker.txt

      # ---------- Commit & push data ----------
      - name: Commit & push data directory
        if: always()
        shell: bash
        run: |
          set -euo pipefail
          git config --global user.name  "github-actions[bot]"
          git config --global user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git config --global --add safe.directory "$GITHUB_WORKSPACE"
          mkdir -p data || true
          git add -f data/** || true
          git add -f data/*  || true
          git add -f apps/web/public/data/** || true
          if git diff --cached --quiet; then
            echo "No changes to commit."
          else
            git commit -m "chore(data): one-ticker batch (sequential, overwrite; skip A & AAPL) [skip ci]"
            git pull --rebase || true
            git push || true
          fi

      # ---------- Node & web build ----------
      - name: Setup Node.js (with npm cache)
        if: ${{ hashFiles('apps/web/package-lock.json') != '' }}
        uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: "npm"
          cache-dependency-path: apps/web/package-lock.json

      - name: Setup Node.js (no cache)
        if: ${{ hashFiles('apps/web/package-lock.json') == '' }}
        uses: actions/setup-node@v4
        with:
          node-version: "20"

      - name: Install web deps
        working-directory: apps/web
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p public/data public/data/ticker
          if [ -f package-lock.json ]; then
            npm ci --no-audit --no-fund
          else
            npm install --no-audit --no-fund || true
          fi
          npm i -D @types/react @types/node --no-audit --no-fund || true

      - name: Build static site and synthesize out/
        working-directory: apps/web
        env:
          NEXT_TELEMETRY_DISABLED: "1"
          NEXT_PUBLIC_BASE_PATH: ${{ env.NEXT_PUBLIC_BASE_PATH }}
        shell: bash
        run: |
          set -euo pipefail
          npm run build || true

          rm -rf out
          mkdir -p out

          if [ -d ".next/static" ]; then
            mkdir -p out/_next
            rsync -a .next/static/ out/_next/static/
          fi

          if [ -d "public" ]; then
            rsync -a public/ out/
          else
            mkdir -p public
            rsync -a public/ out/
          fi

          if [ -d ".next/server/app" ]; then
            find .next/server/app -type f -name 'index.html' -print0 | while IFS= read -r -d '' f; do
              rel="${f#.next/server/app}"
              dir="${rel%/index.html}"
              mkdir -p "out/${dir}"
              cp "$f" "out/${dir}/index.html"
            done
            find .next/server/app -type f -name '*.html' ! -name 'index.html' -print0 | while IFS= read -r -d '' f; do
              rel="${f#.next/server/app/}"
              rel_noext="${rel%.html}"
              mkdir -p "out/${rel_noext}"
              cp "$f" "out/${rel_noext}/index.html"
            done
          fi

          touch out/.nojekyll
          find out -maxdepth 3 -type f | sort

      - name: Configure Pages
        uses: actions/configure-pages@v5

      - name: Upload artifact for Pages
        uses: actions/upload-pages-artifact@v3
        with:
          path: apps/web/out

  deploy:
    needs: build
    runs-on: ubuntu-latest
    environment:
      name: github-pages
    steps:
      - name: Deploy to GitHub Pages
        uses: actions/deploy-pages@v4

  # --------- Self-queue next run (only if more remain) ---------
  queue_next:
    needs: [build, deploy]
    runs-on: ubuntu-latest
    steps:
      - name: Decide if more remain
        id: more
        shell: python
        run: |
          from pathlib import Path
          SKIP = {"A", "AAPL"}
          sp = Path("data/sp500.csv")
          tickers=[]
          if sp.exists():
              for line in sp.read_text(encoding="utf-8").splitlines()[1:]:
                  t=line.strip().upper().replace(".","-")
                  if t and t not in SKIP: tickers.append(t)

          last = Path("data/_last_ticker.txt").read_text(encoding="utf-8").strip().upper() if Path("data/_last_ticker.txt").exists() else ""
          has_more = False
          if tickers:
              try:
                  idx = tickers.index(last) + 1 if last in tickers else 0
                  has_more = idx < len(tickers)
              except ValueError:
                  has_more = True

          print("HAS_MORE:", has_more)
          with open("/home/runner/work/_temp/_out", "w") as f:
              f.write(f"has_more={'true' if has_more else 'false'}\n")

        # expose output
      - name: Export output
        id: export
        run: |
          cat /home/runner/work/_temp/_out >> "$GITHUB_OUTPUT"

      - name: Trigger next workflow run
        if: steps.export.outputs.has_more == 'true'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          OWNER: ${{ github.repository_owner }}
          REPO: ${{ github.event.repository.name }}
        run: |
          set -e
          curl -s -X POST \
            -H "Authorization: Bearer ${GH_TOKEN}" \
            -H "Accept: application/vnd.github+json" \
            https://api.github.com/repos/${OWNER}/${REPO}/actions/workflows/pipeline.yml/dispatches \
            -d '{"ref":"main"}'
